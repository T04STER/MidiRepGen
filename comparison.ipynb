{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3c299b",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13035c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.common.metrics.fid import calculate_fid\n",
    "from src.models.diffusion.ddpm_trainer import DDPMTrainer\n",
    "from src.models.diffusion.ddpm import DDPM\n",
    "from src.models.representation.ae.auto_encoder import Autoencoder, Decoder, Encoder\n",
    "from src.common.diagnostic.summary import show_summary\n",
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d1ff2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_dataset = pickle.load(open(\"data/preprocessed_note_events.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02a29418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[67.0000,  0.4016,  1.6628,  0.6615],\n",
      "        [69.0000,  0.4724,  0.6497,  0.2812],\n",
      "        [70.0000,  0.5354,  0.2669,  0.8659],\n",
      "        [69.0000,  0.3780,  0.8190,  0.1315],\n",
      "        [67.0000,  0.4803,  0.1211,  0.6797],\n",
      "        [67.0000,  0.4094,  0.6849,  0.1497],\n",
      "        [74.0000,  0.4882,  0.0898,  1.7865],\n",
      "        [55.0000,  0.3937,  1.7904,  0.4648]])\n"
     ]
    }
   ],
   "source": [
    "print(events_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f93b86",
   "metadata": {},
   "source": [
    "## DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0901539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from models\\denoisers\\ae\\ddpm_midi_autoencoder\\ddpm_midi_autoencoder.pth\\ddpm_midi_autoencoder.pth\n",
      "Model loaded from ./models/denoisers/ae/ddpm_midi_autoencoder/ddpm_midi_autoencoder.pth/ddpm_midi_autoencoder.pth\n",
      "Autoencoder(\n",
      "  (encoder): Encoder(\n",
      "    (diff_timestep_embedding): Embedding(1000, 128)\n",
      "    (lstm): LSTM(4, 128, num_layers=4, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "    (linear): Linear(in_features=256, out_features=64, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (lstm): LSTM(4, 128, num_layers=4, batch_first=True)\n",
      "    (mom): MemoryOverwriteModule(\n",
      "      (forget_gate): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "      (overwrite_sig): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "      (overwrite_tanh): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (latent_to_hidden): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (latent_to_cell): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (fc_out): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Model: Autoencoder\n",
      "Number of parameters: 2 015 560\n",
      "Number of trainable parameters: 2 015 560\n",
      "Total parameter memory: 7.69 MB\n",
      "Input shape: torch.Size([8, 4])\n",
      "Batch size: 64\n",
      "Dataset size: 4 359 929 samples\n",
      "Parameter to sample ratio: 0.46\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim=4,\n",
    "    hidden_dim=128,\n",
    "    latent_dim=64,\n",
    "    num_layers=4\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    latent_dim=64,\n",
    "    hidden_dim=128,\n",
    "    num_layers=4,\n",
    "    output_dim=4\n",
    ")\n",
    "\n",
    "ae_model = Autoencoder(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    ").to(DEVICE)\n",
    "\n",
    "ddpm = DDPM(1_000)\n",
    "\n",
    "trainer = DDPMTrainer(\n",
    "    model=ae_model,\n",
    "    optimizer=None,\n",
    "    diffusion=ddpm,\n",
    "    run_name=None,\n",
    ")\n",
    "\n",
    "trainer.load_model(\n",
    "    f\"./models/denoisers/ae/ddpm_midi_autoencoder/ddpm_midi_autoencoder.pth/ddpm_midi_autoencoder.pth\",\n",
    ")\n",
    "\n",
    "show_summary(ae_model, input_shape=events_dataset[0].shape, batch_size=BATCH_SIZE, dataset=events_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41fb876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import numpy as np\n",
    "\n",
    "@torch.inference_mode\n",
    "def note_events_to_pretty_midi(note_array: torch.Tensor | np.ndarray, path=\"eg.mid\", default_program=0):\n",
    "    if isinstance(note_array, torch.Tensor):\n",
    "        note_array = note_array.detach().cpu().numpy()\n",
    "    #scale  and velocities\n",
    "    note_array[:, 1] *= 127\n",
    "    # clamp pitches and velocities\n",
    "    note_array[:, 0] = np.clip(note_array[:, 0], 0, 127)\n",
    "    note_array[:, 1] = np.clip(note_array[:, 1], 0, 127)\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(program=default_program)\n",
    "\n",
    "    current_time = 0.0\n",
    "    # print(note_array.shape)\n",
    "    for row in note_array:\n",
    "        pitch, velocity, delta, duration = row\n",
    "        current_time += delta\n",
    "        start = current_time\n",
    "        end = start + duration\n",
    "\n",
    "        note = pretty_midi.Note(\n",
    "            velocity=int(velocity),\n",
    "            pitch=int(pitch),\n",
    "            start=start,\n",
    "            end=end\n",
    "        )\n",
    "        instrument.notes.append(note)\n",
    "\n",
    "    pm.instruments.append(instrument)\n",
    "    pm.write(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "621e9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import write\n",
    "\n",
    "def midiToWav(midi_path, wav_path):\n",
    "    \"\"\"\n",
    "    Convert MIDI file to WAV file using pretty_midi.\n",
    "    \"\"\"\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
    "    audio_data = midi_data.fluidsynth()\n",
    "    write(wav_path, 44100, audio_data.astype(np.float32))\n",
    "\n",
    "@torch.inference_mode()\n",
    "def sampler(model, diffusion: DDPM, noise,) -> np.ndarray:\n",
    "    model.eval()\n",
    "    samples = diffusion.p_sample_loop(model, noise, clip=True)\n",
    "    return samples\n",
    "\n",
    "def diff_adapter(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Adapter function to convert the output of the model to the expected format.\n",
    "    \"\"\"\n",
    "    tensor[:, :, 0] = (tensor[:, :, 0] + 1) * 63.5\n",
    "    tensor[:, :, 1:] = (tensor[:, :, 1:] + 1) / 2\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb32ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "OUTPUT_DATA_PATH = \"data/output\"\n",
    "\n",
    "REAL_MIDI_PATH = f\"{OUTPUT_DATA_PATH}/real_midi\"\n",
    "REAL_WAV_PATH = f\"{OUTPUT_DATA_PATH}/real_wav\"\n",
    "GENERATED_MIDI_PATH = f\"{OUTPUT_DATA_PATH}/generated_midi_ddpm\"\n",
    "GENERATED_WAV_PATH = f\"{OUTPUT_DATA_PATH}/generated_wav_ddpm\"\n",
    "\n",
    "FID_SAMPLE_SIZE = 1000\n",
    "BATCH_SIZE = 64\n",
    "NUM_TIMESTEPS = 1000\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "os.makedirs(REAL_MIDI_PATH, exist_ok=True)\n",
    "os.makedirs(REAL_WAV_PATH, exist_ok=True)\n",
    "os.makedirs(GENERATED_MIDI_PATH, exist_ok=True)\n",
    "os.makedirs(GENERATED_WAV_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9e5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting dataset to WAV: 100%|██████████| 1000/1000 [00:20<00:00, 49.64it/s]\n"
     ]
    }
   ],
   "source": [
    "transformed_real_files = os.listdir(REAL_WAV_PATH)[:FID_SAMPLE_SIZE]\n",
    "dataset_samples = [t for t in events_dataset][:FID_SAMPLE_SIZE]\n",
    "\n",
    "# Convert dataset to WAV\n",
    "if len(dataset_samples) == len(transformed_real_files):\n",
    "    print(\"Dataset already converted to WAV.\")\n",
    "else:\n",
    "    for i, real_sample in enumerate(tqdm(dataset_samples, desc=\"Converting dataset to WAV\")):\n",
    "        midi_path = f\"{REAL_MIDI_PATH}/data_{i}.mid\"\n",
    "        wav_path = f\"{REAL_WAV_PATH}/data_{i}.wav\"\n",
    "        # midiToWav(f\"{REAL_MIDI_PATH}/{midi_path}\", wav_path)\n",
    "        note_events_to_pretty_midi(real_sample, path=midi_path, default_program=0)\n",
    "        midiToWav(midi_path, wav_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "069f3775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating fake samples: 100%|██████████| 16/16 [01:53<00:00,  7.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate fake samples\n",
    "iterations = math.ceil(FID_SAMPLE_SIZE / BATCH_SIZE)\n",
    "\n",
    "midi_count = 0\n",
    "for _ in tqdm(range(iterations), desc=\"Generating fake samples\"):\n",
    "    noise = torch.randn(BATCH_SIZE, 8, 4, device=device)\n",
    "    generated = sampler(\n",
    "        model=ae_model,\n",
    "        diffusion=ddpm,\n",
    "        noise=noise\n",
    "    )\n",
    "    generated = diff_adapter(generated)\n",
    "    \n",
    "    for midi in generated:\n",
    "        midi_path = f\"{GENERATED_MIDI_PATH}/data_{midi_count}.mid\"\n",
    "        wav_path = f\"{GENERATED_WAV_PATH}/data_{midi_count}.wav\"\n",
    "\n",
    "        note_events_to_pretty_midi(midi, path=midi_path, default_program=0)\n",
    "        midiToWav(midi_path,  wav_path)\n",
    "        midi_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04b52c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating FAD for ddpm...\n",
      "FAD score for ddpm: 5.207745154545066\n",
      "FAD scores: {'ddpm': 5.207745154545066}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xconv\\miniconda3\\envs\\midiv3\\lib\\site-packages\\pretty_midi\\instrument.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "Using cache found in C:\\Users\\xconv/.cache\\torch\\hub\\harritaylor_torchvggish_master\n"
     ]
    }
   ],
   "source": [
    "!set PYTHONPATH=%CD% && python ./scripts/calculate_fad.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midiv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

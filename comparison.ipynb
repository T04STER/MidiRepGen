{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3c299b",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13035c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.common.metrics.fid import calculate_fid\n",
    "from src.models.diffusion.ddpm_trainer import DDPMTrainer\n",
    "from src.models.diffusion.ddpm import DDPM\n",
    "from src.models.representation.ae.auto_encoder import Autoencoder, Decoder, Encoder\n",
    "from src.common.diagnostic.summary import show_summary\n",
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d1ff2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_dataset = pickle.load(open(\"data/preprocessed_note_events.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02a29418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[67.0000,  0.4016,  1.6628,  0.6615],\n",
      "        [69.0000,  0.4724,  0.6497,  0.2812],\n",
      "        [70.0000,  0.5354,  0.2669,  0.8659],\n",
      "        [69.0000,  0.3780,  0.8190,  0.1315],\n",
      "        [67.0000,  0.4803,  0.1211,  0.6797],\n",
      "        [67.0000,  0.4094,  0.6849,  0.1497],\n",
      "        [74.0000,  0.4882,  0.0898,  1.7865],\n",
      "        [55.0000,  0.3937,  1.7904,  0.4648]])\n"
     ]
    }
   ],
   "source": [
    "print(events_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f93b86",
   "metadata": {},
   "source": [
    "## DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0901539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from models\\denoisers\\ae\\ddpm_midi_autoencoder\\ddpm_midi_autoencoder.pth\\ddpm_midi_autoencoder.pth\n",
      "Model loaded from ./models/denoisers/ae/ddpm_midi_autoencoder/ddpm_midi_autoencoder.pth/ddpm_midi_autoencoder.pth\n",
      "Autoencoder(\n",
      "  (encoder): Encoder(\n",
      "    (diff_timestep_embedding): Embedding(1000, 128)\n",
      "    (lstm): LSTM(4, 128, num_layers=4, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "    (linear): Linear(in_features=256, out_features=64, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (lstm): LSTM(4, 128, num_layers=4, batch_first=True)\n",
      "    (mom): MemoryOverwriteModule(\n",
      "      (forget_gate): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "      (overwrite_sig): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "      (overwrite_tanh): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (latent_to_hidden): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (latent_to_cell): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (fc_out): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Model: Autoencoder\n",
      "Number of parameters: 2 015 560\n",
      "Number of trainable parameters: 2 015 560\n",
      "Total parameter memory: 7.69 MB\n",
      "Input shape: torch.Size([8, 4])\n",
      "Batch size: 64\n",
      "Dataset size: 4 359 929 samples\n",
      "Parameter to sample ratio: 0.46\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim=4,\n",
    "    hidden_dim=128,\n",
    "    latent_dim=64,\n",
    "    num_layers=4\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    latent_dim=64,\n",
    "    hidden_dim=128,\n",
    "    num_layers=4,\n",
    "    output_dim=4\n",
    ")\n",
    "\n",
    "ae_model = Autoencoder(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    ").to(DEVICE)\n",
    "\n",
    "diffusion = DDPM(1_000)\n",
    "\n",
    "trainer = DDPMTrainer(\n",
    "    model=ae_model,\n",
    "    optimizer=None,\n",
    "    diffusion=diffusion,\n",
    "    run_name=None,\n",
    ")\n",
    "\n",
    "trainer.load_model(\n",
    "    f\"./models/denoisers/ae/ddpm_midi_autoencoder/ddpm_midi_autoencoder.pth/ddpm_midi_autoencoder.pth\",\n",
    ")\n",
    "\n",
    "show_summary(ae_model, input_shape=events_dataset[0].shape, batch_size=BATCH_SIZE, dataset=events_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "from scipy.io.wavfile import write\n",
    "import numpy as np\n",
    "\n",
    "def midiToWav(midi_path, wav_path):\n",
    "    \"\"\"\n",
    "    Convert MIDI file to WAV file using pretty_midi.\n",
    "    \"\"\"\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
    "    audio_data = midi_data.fluidsynth()\n",
    "    write(wav_path, 44100, audio_data.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb32ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "OUTPUT_DATA_PATH = \"data/output\"\n",
    "REAL_PAHT = f\"{OUTPUT_DATA_PATH}/real\"\n",
    "GENERATED_PATH = f\"{OUTPUT_DATA_PATH}/generated\"\n",
    "MIDI_PATH = f\"{OUTPUT_DATA_PATH}/midi\"\n",
    "\n",
    "REAL_MIDI_PATH = \"data/midi_dataset/midis\"\n",
    "\n",
    "FID_SAMPLE_SIZE = 1000\n",
    "BATCH_SIZE = 64\n",
    "NUM_TIMESTEPS = 1000\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "os.makedirs(REAL_PAHT, exist_ok=True)\n",
    "os.makedirs(GENERATED_PATH, exist_ok=True)\n",
    "os.makedirs(MIDI_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9e5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting MIDI to WAV:   0%|          | 0/10854 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting MIDI to WAV:   0%|          | 1/10854 [00:00<1:27:11,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting MIDI to WAV:   0%|          | 2/10854 [00:04<7:23:46,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting MIDI to WAV:   0%|          | 3/10854 [00:04<4:46:38,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting MIDI to WAV:   0%|          | 4/10854 [00:08<7:23:26,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting MIDI to WAV:   0%|          | 5/10854 [00:09<5:37:50,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting MIDI to WAV:   0%|          | 6/10854 [00:10<4:22:54,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting MIDI to WAV:   0%|          | 7/10854 [00:12<5:17:37,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     10\u001b[0m wav_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREAL_PAHT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mmidiToWav\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mREAL_MIDI_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmidi_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwav_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m, in \u001b[0;36mmidiToWav\u001b[1;34m(midi_path, wav_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmidiToWav\u001b[39m(midi_path, wav_path):\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    Convert MIDI file to WAV file using pretty_midi.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     midi_data \u001b[38;5;241m=\u001b[39m \u001b[43mpretty_midi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPrettyMIDI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmidi_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     audio_data \u001b[38;5;241m=\u001b[39m midi_data\u001b[38;5;241m.\u001b[39mfluidsynth()\n\u001b[0;32m     11\u001b[0m     write(wav_path, \u001b[38;5;241m44100\u001b[39m, audio_data\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n",
      "File \u001b[1;32mc:\\Users\\xconv\\miniconda3\\envs\\midiv3\\lib\\site-packages\\pretty_midi\\pretty_midi.py:91\u001b[0m, in \u001b[0;36mPrettyMIDI.__init__\u001b[1;34m(self, midi_file, resolution, initial_tempo)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMIDI file has a largest tick of \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     88\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m it is likely corrupt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(max_tick)))\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Create list that maps ticks to time in seconds\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_tick_to_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_tick\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Populate the list of key and time signature changes\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_metadata(midi_data)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# transform real dataset to .wav\n",
    "real_files = [f for f in os.listdir(REAL_MIDI_PATH) if f.endswith('.mid')][:FID_SAMPLE_SIZE]\n",
    "transformed_real_files = os.listdir(REAL_PAHT)[:FID_SAMPLE_SIZE]\n",
    "\n",
    "if len(real_files) == len(transformed_real_files):\n",
    "    print(\"Real data already converted to WAV.\")\n",
    "else:\n",
    "    for i, midi_path in enumerate(tqdm(real_files, desc=\"Converting MIDI to WAV\")):\n",
    "        wav_path = f\"{REAL_PAHT}/data_{i}.wav\"\n",
    "        midiToWav(f\"{REAL_MIDI_PATH}/{midi_path}\", wav_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f3775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fake samples\n",
    "iterations = math.ceil(FID_SAMPLE_SIZE / BATCH_SIZE)\n",
    "\n",
    "midi_count = 0\n",
    "for _ in range(iterations):\n",
    "    noise = torch.randn(BATCH_SIZE, 3, 32, 32, device=device)\n",
    "    generated = sampler.p_sample_loop(model, noise, num_inference_steps=NUM_TIMESTEPS, clip=True, quiet=True)\n",
    "    generated = torch.from_numpy(generated)\n",
    "    \n",
    "    for midi in generated:\n",
    "        midi = midi.cpu().numpy()\n",
    "        note_events_to_pretty_midi(midi, path=f\"{MIDI_PATH}/sample_{midi_count}.mid\", default_program=0)\n",
    "        midi_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %pip install numba==0.48.0\n",
    "\n",
    "# # Download and load a popular pretrained encoder for symbolic music (MusicVAE - Magenta)\n",
    "# import urllib.request\n",
    "# import os\n",
    "\n",
    "# encoder_path = './models/magenta/cat-mel_2bar_small.tar'\n",
    "# # if not os.path.exists(encoder_path):\n",
    "# #     url = 'https://huggingface.co/magenta/music-vae/resolve/main/mel_2bar_small.pth'  # Example: MusicVAE 2-bar melody encoder\n",
    "# #     urllib.request.urlretrieve(url, encoder_path)\n",
    "\n",
    "# # MusicVAE encoder architecture (simplified, adjust as needed for your use case)\n",
    "# from magenta.models.music_vae.trained_model import TrainedModel\n",
    "\n",
    "# music_vae = TrainedModel(\n",
    "#     config='cat-mel_2bar_small',\n",
    "#     batch_size=64,\n",
    "#     checkpoint_dir_or_path=encoder_path\n",
    "# )\n",
    "# pretrained_encoder = music_vae._config.data_converter\n",
    "# # Note: You may need to adapt your pipeline to use the MusicVAE encoder output for FID calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8295fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fid = calculate_fid(\n",
    "#     model=ae_model,\n",
    "#     sampler=trainer.sampler,\n",
    "#     encoder=pretrained_encoder, # use pretrained encoder for FID calculation\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeefd83",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2537d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midiv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
